# ‚öôÔ∏è Sistema de Web Scraping

Este directorio contiene las implementaciones concretas de los `scrapers` para las diferentes tiendas online. Su responsabilidad es navegar por los sitios web externos, extraer informaci√≥n de los productos y devolverla en un formato estructurado que el resto de la aplicaci√≥n pueda procesar.

Esta capa es una implementaci√≥n de la l√≥gica definida en el caso de uso de scraping y es invocada peri√≥dicamente por el planificador de tareas.

---

## üèóÔ∏è Estructura y Scrapers Implementados

El sistema est√° dise√±ado para ser modular, donde cada scraper es un componente independiente enfocado en una √∫nica tienda.

| Archivo            | Tienda   | Descripci√≥n                                                                                                                              |
| :----------------- | :------- | :--------------------------------------------------------------------------------------------------------------------------------------- |
| **`aussar.go`**    | Aussar   | Implementa el scraping para Aussar.es. Mapea categor√≠as internas a URLs de la tienda y extrae la informaci√≥n b√°sica de los listados.          |
| **`coolmod.go`**   | Coolmod  | Implementa el scraping para Coolmod.com. Maneja la estructura espec√≠fica de su cat√°logo y la forma en que presentan los precios.            |
| **`ebay.go`**      | eBay     | Implementa el scraping para eBay.com. Incluye l√≥gica avanzada para manejar la variabilidad de los listados y extraer im√°genes de alta calidad, evitando los *placeholders* comunes de la plataforma. |

<br/>

> üìö **Nota sobre la implementaci√≥n:** Todos los scrapers utilizan la biblioteca [**Colly**](https://github.com/gocolly/colly), un framework de scraping r√°pido y elegante para Go.

---

## ‚öôÔ∏è Funcionamiento del Sistema

El proceso de scraping es una tarea fundamental que se ejecuta en segundo plano para mantener la base de datos de productos actualizada.

1.  **Invocaci√≥n Programada**: El planificador de tareas, definido en `internal/interface/cron/readme.md`, invoca al `ScraperUseCase` cada 48 horas.

2.  **Ejecuci√≥n por Categor√≠a**: El `ScraperUseCase` itera sobre todas las categor√≠as activas del sistema (Port√°tiles, Tarjetas Gr√°ficas, etc.) y ejecuta cada uno de los scrapers implementados para esa categor√≠a.

3.  **Extracci√≥n de Datos**: Cada scraper visita la URL correspondiente de la tienda y extrae una lista de productos con su informaci√≥n esencial:
    -   Nombre del producto
    -   Precio
    -   URL de la p√°gina de detalle
    -   URL de la imagen

4.  **Validaci√≥n de Relevancia**: Una vez que un scraper devuelve una lista de productos, estos se pasan por un validador (`pkg/utils/category_validator.go`). Este es un paso **cr√≠tico** que utiliza un sistema de palabras clave para asegurar que un producto extra√≠do (ej: "funda para port√°til") no sea incorrectamente asignado a una categor√≠a principal (ej: "Port√°tiles"). Esto garantiza una alta calidad y relevancia de los datos. Para m√°s detalles, consulta la documentaci√≥n en `pkg/utils/readme.md`.

5.  **Persistencia de Datos**: Los productos validados son procesados por el `ScraperUseCase` para ser guardados en la base de datos. El sistema comprueba si el producto ya existe para actualizar su precio, o lo crea si es nuevo.

---

